{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "794980ba379e4b599ef2787c5c7f8a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1dd83687e64486e8f17ef9a135389ec",
              "IPY_MODEL_dea8b0265e5245268553e6239bb50646",
              "IPY_MODEL_eeb1b17f59944675958d4d4d45541e39"
            ],
            "layout": "IPY_MODEL_177c95e450af47db848f5dd8f95c538e"
          }
        },
        "f1dd83687e64486e8f17ef9a135389ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2806c6c1dab74ccaa76dcdbcca1d03ec",
            "placeholder": "​",
            "style": "IPY_MODEL_6085dee6e3464ac8b79654e9c595a305",
            "value": "100%"
          }
        },
        "dea8b0265e5245268553e6239bb50646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_396863b1cda84846a33df00aa0c55b84",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f36fffa1e6d9488ebaf2d42b6be50112",
            "value": 4
          }
        },
        "eeb1b17f59944675958d4d4d45541e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df8b71412c0d413d8d2c4cd09636d37d",
            "placeholder": "​",
            "style": "IPY_MODEL_2a0466f1ab434f1caa15c7f0b9091a22",
            "value": " 4/4 [00:00&lt;00:00, 26.97it/s]"
          }
        },
        "177c95e450af47db848f5dd8f95c538e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2806c6c1dab74ccaa76dcdbcca1d03ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6085dee6e3464ac8b79654e9c595a305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "396863b1cda84846a33df00aa0c55b84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36fffa1e6d9488ebaf2d42b6be50112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df8b71412c0d413d8d2c4cd09636d37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a0466f1ab434f1caa15c7f0b9091a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bdc4ca4e612470fb334c14f1c6f52bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5990b31bbec8474c89d2b9da95c17c11",
              "IPY_MODEL_740baa4cc8e04333a035c14dd79d81e3",
              "IPY_MODEL_9ad958599f3d48d1a897911077a61470"
            ],
            "layout": "IPY_MODEL_19cfadd4b75d4a5891ba95ba6825a197"
          }
        },
        "5990b31bbec8474c89d2b9da95c17c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863ceb79f6254ef3901945c5ae24626e",
            "placeholder": "​",
            "style": "IPY_MODEL_ec5779060c2242beba5f03d9ae0500ef",
            "value": "100%"
          }
        },
        "740baa4cc8e04333a035c14dd79d81e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f5782df13b4e7c8676621e6a656a72",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7593f64acfa4bcb9e639af4b6c8e6fc",
            "value": 64
          }
        },
        "9ad958599f3d48d1a897911077a61470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_463032f19b234513b2cd46725bfd6ba9",
            "placeholder": "​",
            "style": "IPY_MODEL_52849cde5f694898a8989da9b0c7aca1",
            "value": " 64/64 [00:01&lt;00:00, 65.12it/s]"
          }
        },
        "19cfadd4b75d4a5891ba95ba6825a197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "863ceb79f6254ef3901945c5ae24626e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5779060c2242beba5f03d9ae0500ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2f5782df13b4e7c8676621e6a656a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7593f64acfa4bcb9e639af4b6c8e6fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "463032f19b234513b2cd46725bfd6ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52849cde5f694898a8989da9b0c7aca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgzZWi_cXqBI",
        "outputId": "4b22b1a0-b9a3-4924-8be0-2551f39c87b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! apt -y install gh"
      ],
      "metadata": {
        "id": "nBXN0lsHcJ6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479d03a9-0183-465a-cdd3-dca33c93c331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "gh is already the newest version (2.4.0+dfsg1-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone -b responses https://github.com/lightblue-tech/M-IFEval.git"
      ],
      "metadata": {
        "id": "lL-gwpCxaHn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3e72ea-f28b-40d2-cc38-42d0bca1fb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'M-IFEval'...\n",
            "remote: Enumerating objects: 641, done.\u001b[K\n",
            "remote: Counting objects: 100% (337/337), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 641 (delta 71), reused 312 (delta 56), pack-reused 304 (from 1)\u001b[K\n",
            "Receiving objects: 100% (641/641), 29.55 MiB | 15.89 MiB/s, done.\n",
            "Resolving deltas: 100% (259/259), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response generation"
      ],
      "metadata": {
        "id": "E-_AW2NA-kaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd M-IFEval && pip install -q -r requirements.txt\n",
        "! pip install -q vllm bitsandbytes hf-transfer"
      ],
      "metadata": {
        "id": "a7SInk5O2Zl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa43b03c-58f4-479d-e47b-2b1ebdb8cdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/981.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m870.4/981.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "! huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2mHsnuRjn5I",
        "outputId": "ec8b5af7-4a40-46b7-e716-f19203e9afe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNyBbCTa_LQ2",
        "outputId": "697291a9-01d9-46b3-c5e5-1c87548663f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_model_names = ['CohereForAI/c4ai-command-r-plus-4bit',\n",
        " 'CohereForAI/c4ai-command-r-v01-4bit',\n",
        " 'CohereForAI/aya-23-8B',\n",
        " 'Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4',\n",
        " 'Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4',\n",
        " 'Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4',\n",
        " 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4',\n",
        " 'Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4',\n",
        " 'Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4',\n",
        " 'Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4',\n",
        " 'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4',\n",
        " 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\n",
        " 'mistralai/Mistral-7B-Instruct-v0.3']\n",
        "\n",
        "for local_model_name in local_model_names:\n",
        "    ! cd M-IFEval && HF_HUB_ENABLE_HF_TRANSFER=1 python get_responses.py --model_name {local_model_name}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5tutI3e-4wA",
        "outputId": "73e2eb5e-3021-453b-9b08-338b007c9576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-02 07:05:25.753839: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:05:25.770364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:05:25.791711: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:05:25.798098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:05:25.813480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:05:27.082186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING 10-02 07:05:29 config.py:319] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 10-02 07:05:29 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='CohereForAI/c4ai-command-r-plus-4bit', speculative_config=None, tokenizer='CohereForAI/c4ai-command-r-plus-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=CohereForAI/c4ai-command-r-plus-4bit, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 10-02 07:05:31 model_runner.py:1014] Starting to load model CohereForAI/c4ai-command-r-plus-4bit...\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 179, in <module>\n",
            "[rank0]:     response_generator = model_class(model_name)\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 120, in __init__\n",
            "[rank0]:     self.llm = LLM(model=self.model_name, max_model_len=os.environ.get(\"MAX_MODEL_LEN\", 4096))\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 214, in __init__\n",
            "[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\n",
            "[rank0]:     engine = cls(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 325, in __init__\n",
            "[rank0]:     self.model_executor = executor_class(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\n",
            "[rank0]:     self._init_executor()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 40, in _init_executor\n",
            "[rank0]:     self.driver_worker.load_model()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n",
            "[rank0]:     self.model_runner.load_model()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1016, in load_model\n",
            "[rank0]:     self.model = get_model(model_config=self.model_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n",
            "[rank0]:     return loader.load_model(model_config=model_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 399, in load_model\n",
            "[rank0]:     model = _initialize_model(model_config, self.load_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 176, in _initialize_model\n",
            "[rank0]:     return build_model(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 161, in build_model\n",
            "[rank0]:     return model_class(config=hf_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/commandr.py\", line 335, in __init__\n",
            "[rank0]:     self.model = CohereModel(config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/commandr.py\", line 268, in __init__\n",
            "[rank0]:     self.layers = nn.ModuleList([\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/commandr.py\", line 269, in <listcomp>\n",
            "[rank0]:     CohereDecoderLayer(config, cache_config, quant_config=quant_config)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/commandr.py\", line 223, in __init__\n",
            "[rank0]:     self.mlp = CohereMLP(config, quant_config=quant_config)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/commandr.py\", line 92, in __init__\n",
            "[rank0]:     self.gate_up_proj = MergedColumnParallelLinear(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 420, in __init__\n",
            "[rank0]:     super().__init__(input_size=input_size,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 304, in __init__\n",
            "[rank0]:     self.quant_method.create_weights(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 189, in create_weights\n",
            "[rank0]:     qweight = create_qweight_for_4bit()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 173, in create_qweight_for_4bit\n",
            "[rank0]:     qweight = torch.nn.Parameter(torch.empty(total_size // quant_ratio,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 79, in __torch_function__\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 72.81 MiB is free. Process 137613 has 39.48 GiB memory in use. Of the allocated memory 39.00 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-02 07:05:40.388559: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:05:40.404622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:05:40.426582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:05:40.433354: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:05:40.448601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:05:41.713508: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING 10-02 07:05:44 config.py:319] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 10-02 07:05:44 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='CohereForAI/c4ai-command-r-v01-4bit', speculative_config=None, tokenizer='CohereForAI/c4ai-command-r-v01-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=CohereForAI/c4ai-command-r-v01-4bit, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 10-02 07:05:46 model_runner.py:1014] Starting to load model CohereForAI/c4ai-command-r-v01-4bit...\n",
            "INFO 10-02 07:05:48 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards:   0% 0/5 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 179, in <module>\n",
            "[rank0]:     response_generator = model_class(model_name)\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 120, in __init__\n",
            "[rank0]:     self.llm = LLM(model=self.model_name, max_model_len=os.environ.get(\"MAX_MODEL_LEN\", 4096))\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 214, in __init__\n",
            "[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\n",
            "[rank0]:     engine = cls(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 325, in __init__\n",
            "[rank0]:     self.model_executor = executor_class(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\n",
            "[rank0]:     self._init_executor()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 40, in _init_executor\n",
            "[rank0]:     self.driver_worker.load_model()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n",
            "[rank0]:     self.model_runner.load_model()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1016, in load_model\n",
            "[rank0]:     self.model = get_model(model_config=self.model_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n",
            "[rank0]:     return loader.load_model(model_config=model_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 403, in load_model\n",
            "[rank0]:     model.load_weights(self._get_all_weights(model_config, model))\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/commandr.py\", line 408, in load_weights\n",
            "[rank0]:     param = params_dict[name]\n",
            "[rank0]: KeyError: 'model.layers.0.mlp.down_proj.weight'\n",
            "Loading safetensors checkpoint shards:   0% 0/5 [00:02<?, ?it/s]\n",
            "2024-10-02 07:05:57.670082: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:05:57.686565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:05:57.708426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:05:57.714945: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:05:57.730530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:05:59.025349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 10-02 07:06:01 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='CohereForAI/aya-23-8B', speculative_config=None, tokenizer='CohereForAI/aya-23-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=CohereForAI/aya-23-8B, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 10-02 07:06:03 model_runner.py:1014] Starting to load model CohereForAI/aya-23-8B...\n",
            "INFO 10-02 07:06:05 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 4/4 [00:05<00:00,  1.28s/it]\n",
            "INFO 10-02 07:06:12 model_runner.py:1025] Loading model weights took 14.9553 GB\n",
            "INFO 10-02 07:06:15 gpu_executor.py:122] # GPU blocks: 9079, # CPU blocks: 2048\n",
            "INFO 10-02 07:06:17 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:06:17 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:06:45 model_runner.py:1456] Graph capturing finished in 28 secs.\n",
            "./data/en_input_data.jsonl - CohereForAI/aya-23-8B\n",
            "Generating train split: 541 examples [00:00, 14539.67 examples/s]\n",
            "Processed prompts: 100% 541/541 [01:15<00:00,  7.19it/s, est. speed input: 384.32 toks/s, output: 2732.34 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 56.89ba/s]\n",
            "./data/es_input_data.jsonl - CohereForAI/aya-23-8B\n",
            "Generating train split: 115 examples [00:00, 27220.37 examples/s]\n",
            "Processed prompts: 100% 115/115 [00:38<00:00,  2.98it/s, est. speed input: 131.86 toks/s, output: 1353.20 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 219.79ba/s]\n",
            "./data/fr_input_data.jsonl - CohereForAI/aya-23-8B\n",
            "Generating train split: 235 examples [00:00, 37955.31 examples/s]\n",
            "Processed prompts: 100% 235/235 [00:49<00:00,  4.73it/s, est. speed input: 297.66 toks/s, output: 2362.27 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 126.70ba/s]\n",
            "./data/ja_input_data.jsonl - CohereForAI/aya-23-8B\n",
            "Generating train split: 172 examples [00:00, 37595.51 examples/s]\n",
            "Processed prompts: 100% 172/172 [00:42<00:00,  4.00it/s, est. speed input: 207.76 toks/s, output: 1865.35 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 121.82ba/s]\n",
            "2024-10-02 07:10:23.887661: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:10:23.904804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:10:23.926895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:10:23.933450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:10:23.949365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:10:25.217904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 10-02 07:10:27 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:10:27 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 10-02 07:10:29 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:10:29 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:10:30 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "INFO 10-02 07:10:30 weight_utils.py:287] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:00<00:00,  4.37it/s]\n",
            "INFO 10-02 07:10:31 model_runner.py:1025] Loading model weights took 0.4301 GB\n",
            "INFO 10-02 07:10:32 gpu_executor.py:122] # GPU blocks: 183170, # CPU blocks: 21845\n",
            "INFO 10-02 07:10:35 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:10:35 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:10:57 model_runner.py:1456] Graph capturing finished in 22 secs.\n",
            "./data/en_input_data.jsonl - Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 541/541 [01:21<00:00,  6.61it/s, est. speed input: 498.51 toks/s, output: 5820.02 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 43.04ba/s]\n",
            "./data/es_input_data.jsonl - Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 115/115 [00:18<00:00,  6.34it/s, est. speed input: 455.07 toks/s, output: 5929.89 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 162.84ba/s]\n",
            "./data/fr_input_data.jsonl - Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 235/235 [00:39<00:00,  5.91it/s, est. speed input: 540.03 toks/s, output: 6911.42 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 71.53ba/s]\n",
            "./data/ja_input_data.jsonl - Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 172/172 [00:25<00:00,  6.75it/s, est. speed input: 541.74 toks/s, output: 6230.52 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 91.70ba/s]\n",
            "2024-10-02 07:13:57.838208: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:13:57.854580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:13:57.876080: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:13:57.882508: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:13:57.897940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:13:59.173706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 10-02 07:14:01 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:14:01 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 10-02 07:14:03 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:14:03 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:14:04 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model.safetensors: 100% 1.15G/1.15G [00:02<00:00, 566MB/s] \n",
            "INFO 10-02 07:14:06 weight_utils.py:287] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:00<00:00,  2.31it/s]\n",
            "INFO 10-02 07:14:07 model_runner.py:1025] Loading model weights took 1.0975 GB\n",
            "INFO 10-02 07:14:08 gpu_executor.py:122] # GPU blocks: 76627, # CPU blocks: 9362\n",
            "INFO 10-02 07:14:12 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:14:12 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:14:35 model_runner.py:1456] Graph capturing finished in 23 secs.\n",
            "./data/en_input_data.jsonl - Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 541/541 [00:53<00:00, 10.18it/s, est. speed input: 767.22 toks/s, output: 4783.40 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 71.06ba/s]\n",
            "./data/es_input_data.jsonl - Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 115/115 [00:19<00:00,  5.84it/s, est. speed input: 419.07 toks/s, output: 3607.42 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 228.70ba/s]\n",
            "./data/fr_input_data.jsonl - Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 235/235 [00:35<00:00,  6.71it/s, est. speed input: 613.25 toks/s, output: 4933.66 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 98.66ba/s]\n",
            "./data/ja_input_data.jsonl - Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 172/172 [00:26<00:00,  6.37it/s, est. speed input: 511.53 toks/s, output: 4405.91 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 115.66ba/s]\n",
            "2024-10-02 07:17:03.062736: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:17:03.079014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:17:03.100472: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:17:03.106904: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:17:03.122461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:17:04.428246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.26k/1.26k [00:00<00:00, 8.81MB/s]\n",
            "INFO 10-02 07:17:07 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:17:07 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 32.4MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 4.32MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 3.79MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 13.7MB/s]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.80MB/s]\n",
            "INFO 10-02 07:17:13 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:17:13 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:17:14 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model.safetensors: 100% 2.07G/2.07G [00:10<00:00, 206MB/s] \n",
            "INFO 10-02 07:17:25 weight_utils.py:287] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:00<00:00,  1.39it/s]\n",
            "INFO 10-02 07:17:26 model_runner.py:1025] Loading model weights took 1.9449 GB\n",
            "INFO 10-02 07:17:27 gpu_executor.py:122] # GPU blocks: 58109, # CPU blocks: 7281\n",
            "INFO 10-02 07:17:30 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:17:30 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:17:54 model_runner.py:1456] Graph capturing finished in 24 secs.\n",
            "./data/en_input_data.jsonl - Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 541/541 [00:56<00:00,  9.59it/s, est. speed input: 722.90 toks/s, output: 4198.87 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 69.82ba/s]\n",
            "./data/es_input_data.jsonl - Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 115/115 [00:24<00:00,  4.71it/s, est. speed input: 338.10 toks/s, output: 3071.71 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 250.83ba/s]\n",
            "./data/fr_input_data.jsonl - Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 235/235 [00:34<00:00,  6.79it/s, est. speed input: 621.30 toks/s, output: 4288.80 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 113.66ba/s]\n",
            "./data/ja_input_data.jsonl - Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 172/172 [00:26<00:00,  6.56it/s, est. speed input: 526.77 toks/s, output: 3391.90 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 154.57ba/s]\n",
            "2024-10-02 07:20:27.947278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:20:27.964028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:20:27.986453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:20:27.993105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:20:28.008496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:20:29.292662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.26k/1.26k [00:00<00:00, 6.42MB/s]\n",
            "INFO 10-02 07:20:32 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:20:32 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "tokenizer_config.json: 100% 7.22k/7.22k [00:00<00:00, 29.7MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 3.19MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 7.13MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 28.1MB/s]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.80MB/s]\n",
            "INFO 10-02 07:20:38 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:20:38 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:20:38 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model-00001-of-00002.safetensors: 100% 4.00G/4.00G [00:11<00:00, 336MB/s]\n",
            "model-00002-of-00002.safetensors: 100% 1.58G/1.58G [00:05<00:00, 313MB/s] \n",
            "model.safetensors.index.json: 100% 75.4k/75.4k [00:00<00:00, 207MB/s]\n",
            "Loading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.14it/s]\n",
            "INFO 10-02 07:20:59 model_runner.py:1025] Loading model weights took 5.1810 GB\n",
            "INFO 10-02 07:21:00 gpu_executor.py:122] # GPU blocks: 33003, # CPU blocks: 4681\n",
            "INFO 10-02 07:21:04 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:21:04 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:21:27 model_runner.py:1456] Graph capturing finished in 23 secs.\n",
            "./data/en_input_data.jsonl - Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 541/541 [00:48<00:00, 11.19it/s, est. speed input: 731.66 toks/s, output: 3383.95 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 88.01ba/s]\n",
            "./data/es_input_data.jsonl - Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 115/115 [00:20<00:00,  5.53it/s, est. speed input: 341.42 toks/s, output: 2021.67 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 331.36ba/s]\n",
            "./data/fr_input_data.jsonl - Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 235/235 [00:28<00:00,  8.20it/s, est. speed input: 668.06 toks/s, output: 3159.09 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 157.53ba/s]\n",
            "./data/ja_input_data.jsonl - Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 172/172 [00:24<00:00,  7.05it/s, est. speed input: 495.41 toks/s, output: 2584.54 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 151.39ba/s]\n",
            "2024-10-02 07:23:42.974561: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:23:42.991474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:23:43.013888: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:23:43.020580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:23:43.036749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:23:44.356184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.26k/1.26k [00:00<00:00, 8.00MB/s]\n",
            "INFO 10-02 07:23:47 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:23:47 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 30.2MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 6.22MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 3.85MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:01<00:00, 6.44MB/s]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.68MB/s]\n",
            "INFO 10-02 07:23:53 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:23:53 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:23:54 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model-00001-of-00003.safetensors: 100% 3.99G/3.99G [00:10<00:00, 371MB/s]\n",
            "model-00002-of-00003.safetensors: 100% 3.97G/3.97G [00:10<00:00, 371MB/s]\n",
            "model-00003-of-00003.safetensors: 100% 2.02G/2.02G [00:08<00:00, 230MB/s]\n",
            "model.safetensors.index.json: 100% 129k/129k [00:00<00:00, 611kB/s]\n",
            "Loading safetensors checkpoint shards: 100% 3/3 [00:03<00:00,  1.02s/it]\n",
            "INFO 10-02 07:24:31 model_runner.py:1025] Loading model weights took 9.3323 GB\n",
            "INFO 10-02 07:24:32 gpu_executor.py:122] # GPU blocks: 8274, # CPU blocks: 1365\n",
            "INFO 10-02 07:24:35 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:24:35 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:25:01 model_runner.py:1456] Graph capturing finished in 25 secs.\n",
            "./data/en_input_data.jsonl - Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 541/541 [01:22<00:00,  6.58it/s, est. speed input: 496.30 toks/s, output: 2083.12 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 82.14ba/s]\n",
            "./data/es_input_data.jsonl - Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 115/115 [00:22<00:00,  5.10it/s, est. speed input: 365.88 toks/s, output: 1929.14 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 325.57ba/s]\n",
            "./data/fr_input_data.jsonl - Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 235/235 [00:48<00:00,  4.81it/s, est. speed input: 439.55 toks/s, output: 1886.96 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 170.85ba/s]\n",
            "./data/ja_input_data.jsonl - Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 172/172 [00:39<00:00,  4.34it/s, est. speed input: 348.76 toks/s, output: 1539.45 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 157.04ba/s]\n",
            "2024-10-02 07:28:26.557262: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:28:26.573836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:28:26.595549: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:28:26.602086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:28:26.618114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:28:27.937627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 10-02 07:28:30 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:28:30 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 10-02 07:28:32 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:28:32 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:28:33 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model-00004-of-00005.safetensors: 100% 3.98G/3.98G [00:11<00:00, 336MB/s]\n",
            "model.safetensors.index.json: 100% 172k/172k [00:00<00:00, 797kB/s]\n",
            "Loading safetensors checkpoint shards: 100% 5/5 [01:12<00:00, 14.40s/it]\n",
            "INFO 10-02 07:29:59 model_runner.py:1025] Loading model weights took 18.0192 GB\n",
            "INFO 10-02 07:30:01 gpu_executor.py:122] # GPU blocks: 3885, # CPU blocks: 1024\n",
            "INFO 10-02 07:30:04 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:30:04 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:30:31 model_runner.py:1456] Graph capturing finished in 27 secs.\n",
            "./data/en_input_data.jsonl - Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
            "Processed prompts:  33% 179/541 [00:41<01:22,  4.38it/s, est. speed input: 337.73 toks/s, output: 413.73 toks/s]WARNING 10-02 07:31:13 scheduler.py:1439] Sequence group 433 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
            "Processed prompts:  56% 305/541 [01:02<00:37,  6.21it/s, est. speed input: 373.63 toks/s, output: 729.17 toks/s]WARNING 10-02 07:31:34 scheduler.py:1439] Sequence group 499 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
            "Processed prompts: 100% 541/541 [02:01<00:00,  4.44it/s, est. speed input: 334.72 toks/s, output: 1230.11 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 90.19ba/s]\n",
            "./data/es_input_data.jsonl - Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 115/115 [00:36<00:00,  3.15it/s, est. speed input: 226.13 toks/s, output: 1199.25 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 333.36ba/s]\n",
            "./data/fr_input_data.jsonl - Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 235/235 [01:24<00:00,  2.78it/s, est. speed input: 254.10 toks/s, output: 1060.55 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 147.08ba/s]\n",
            "./data/ja_input_data.jsonl - Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
            "Processed prompts: 100% 172/172 [01:08<00:00,  2.52it/s, est. speed input: 201.98 toks/s, output: 875.02 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 166.05ba/s]\n",
            "2024-10-02 07:35:53.920230: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:35:53.936826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:35:53.958771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:35:53.965284: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:35:53.981027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:35:55.293620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.26k/1.26k [00:00<00:00, 8.43MB/s]\n",
            "INFO 10-02 07:35:58 gptq_marlin.py:107] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
            "INFO 10-02 07:35:58 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 31.7MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 3.24MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 2.55MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 15.6MB/s]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.71MB/s]\n",
            "INFO 10-02 07:36:04 model_runner.py:1014] Starting to load model Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4...\n",
            "INFO 10-02 07:36:04 gptq_marlin.py:198] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
            "INFO 10-02 07:36:05 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model-00001-of-00011.safetensors: 100% 3.94G/3.94G [00:12<00:00, 316MB/s]\n",
            "model-00002-of-00011.safetensors: 100% 3.92G/3.92G [00:10<00:00, 357MB/s]\n",
            "model-00003-of-00011.safetensors: 100% 4.00G/4.00G [00:15<00:00, 252MB/s] \n",
            "model-00004-of-00011.safetensors: 100% 4.00G/4.00G [00:20<00:00, 191MB/s]\n",
            "model-00005-of-00011.safetensors: 100% 3.92G/3.92G [00:10<00:00, 369MB/s]\n",
            "model-00006-of-00011.safetensors: 100% 4.00G/4.00G [00:15<00:00, 256MB/s]\n",
            "model-00007-of-00011.safetensors: 100% 4.00G/4.00G [00:17<00:00, 231MB/s]\n",
            "model-00008-of-00011.safetensors: 100% 3.92G/3.92G [00:12<00:00, 304MB/s]\n",
            "model-00009-of-00011.safetensors: 100% 4.00G/4.00G [00:15<00:00, 251MB/s]\n",
            "model-00010-of-00011.safetensors: 100% 3.46G/3.46G [00:09<00:00, 358MB/s] \n",
            "model-00011-of-00011.safetensors: 100% 2.49G/2.49G [00:11<00:00, 222MB/s]\n",
            "model.safetensors.index.json: 100% 216k/216k [00:00<00:00, 914kB/s]\n",
            "Loading safetensors checkpoint shards: 100% 11/11 [02:02<00:00, 11.15s/it]\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 179, in <module>\n",
            "[rank0]:     response_generator = model_class(model_name)\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 120, in __init__\n",
            "[rank0]:     self.llm = LLM(model=self.model_name, max_model_len=os.environ.get(\"MAX_MODEL_LEN\", 4096))\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 214, in __init__\n",
            "[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\n",
            "[rank0]:     engine = cls(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 325, in __init__\n",
            "[rank0]:     self.model_executor = executor_class(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\n",
            "[rank0]:     self._init_executor()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 40, in _init_executor\n",
            "[rank0]:     self.driver_worker.load_model()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n",
            "[rank0]:     self.model_runner.load_model()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1016, in load_model\n",
            "[rank0]:     self.model = get_model(model_config=self.model_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n",
            "[rank0]:     return loader.load_model(model_config=model_config,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 414, in load_model\n",
            "[rank0]:     quant_method.process_weights_after_loading(module)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/gptq_marlin.py\", line 296, in process_weights_after_loading\n",
            "[rank0]:     self.kernel.process_weights_after_loading(layer)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/kernels/marlin.py\", line 108, in process_weights_after_loading\n",
            "[rank0]:     self._transform_param(layer, self.w_q_name, transform_w_q)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/kernels/MPLinearKernel.py\", line 64, in _transform_param\n",
            "[rank0]:     new_param = fn(old_param)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/kernels/marlin.py\", line 92, in transform_w_q\n",
            "[rank0]:     x.data = ops.gptq_marlin_repack(x.data.contiguous(),\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 34, in wrapper\n",
            "[rank0]:     return fn(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 537, in gptq_marlin_repack\n",
            "[rank0]:     return torch.ops._C.gptq_marlin_repack(b_q_weight, perm, size_k, size_n,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1061, in __call__\n",
            "[rank0]:     return self_._op(*args, **(kwargs or {}))\n",
            "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 232.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 200.81 MiB is free. Process 281793 has 39.36 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 107.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-02 07:40:55.207188: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:40:55.223690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:40:55.245342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:40:55.251808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:40:55.267175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:40:56.544080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.04k/1.04k [00:00<00:00, 7.06MB/s]\n",
            "INFO 10-02 07:40:59 awq_marlin.py:90] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
            "INFO 10-02 07:40:59 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "tokenizer_config.json: 100% 55.4k/55.4k [00:00<00:00, 83.9MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 10.3MB/s]\n",
            "special_tokens_map.json: 100% 295/295 [00:00<00:00, 2.02MB/s]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.32MB/s]\n",
            "INFO 10-02 07:41:04 model_runner.py:1014] Starting to load model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4...\n",
            "INFO 10-02 07:41:05 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model-00001-of-00009.safetensors: 100% 4.97G/4.97G [00:13<00:00, 362MB/s]\n",
            "model-00002-of-00009.safetensors: 100% 4.89G/4.89G [00:13<00:00, 364MB/s]\n",
            "model-00003-of-00009.safetensors: 100% 4.89G/4.89G [00:13<00:00, 350MB/s] \n",
            "model-00004-of-00009.safetensors: 100% 4.89G/4.89G [00:14<00:00, 329MB/s]\n",
            "model-00005-of-00009.safetensors: 100% 4.89G/4.89G [00:15<00:00, 310MB/s]\n",
            "model-00006-of-00009.safetensors: 100% 4.89G/4.89G [00:12<00:00, 387MB/s] \n",
            "model-00007-of-00009.safetensors: 100% 4.89G/4.89G [00:14<00:00, 334MB/s]\n",
            "model-00008-of-00009.safetensors: 100% 3.36G/3.36G [00:10<00:00, 320MB/s]\n",
            "model-00009-of-00009.safetensors: 100% 2.10G/2.10G [00:06<00:00, 314MB/s]\n",
            "model.safetensors.index.json: 100% 159k/159k [00:00<00:00, 357kB/s]\n",
            "Loading safetensors checkpoint shards: 100% 9/9 [02:36<00:00, 17.44s/it]\n",
            "INFO 10-02 07:45:50 model_runner.py:1025] Loading model weights took 37.0966 GB\n",
            "INFO 10-02 07:45:54 gpu_executor.py:122] # GPU blocks: 0, # CPU blocks: 819\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 179, in <module>\n",
            "[rank0]:     response_generator = model_class(model_name)\n",
            "[rank0]:   File \"/content/M-IFEval/get_responses.py\", line 120, in __init__\n",
            "[rank0]:     self.llm = LLM(model=self.model_name, max_model_len=os.environ.get(\"MAX_MODEL_LEN\", 4096))\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 214, in __init__\n",
            "[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\n",
            "[rank0]:     engine = cls(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 339, in __init__\n",
            "[rank0]:     self._initialize_kv_caches()\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 487, in _initialize_kv_caches\n",
            "[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 125, in initialize_cache\n",
            "[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 258, in initialize_cache\n",
            "[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 478, in raise_if_cache_size_invalid\n",
            "[rank0]:     raise ValueError(\"No available memory for the cache blocks. \"\n",
            "[rank0]: ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
            "2024-10-02 07:46:06.829288: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:46:06.845879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:46:06.867262: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:46:06.873672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:46:06.889080: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:46:08.843522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.04k/1.04k [00:00<00:00, 6.55MB/s]\n",
            "INFO 10-02 07:46:13 awq_marlin.py:90] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
            "INFO 10-02 07:46:13 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "tokenizer_config.json: 100% 55.4k/55.4k [00:00<00:00, 42.2MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:06<00:00, 1.47MB/s]\n",
            "special_tokens_map.json: 100% 295/295 [00:00<00:00, 1.64MB/s]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.16MB/s]\n",
            "INFO 10-02 07:46:23 model_runner.py:1014] Starting to load model hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4...\n",
            "INFO 10-02 07:46:24 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "model-00001-of-00002.safetensors: 100% 4.68G/4.68G [00:14<00:00, 317MB/s]\n",
            "model-00002-of-00002.safetensors: 100% 1.05G/1.05G [00:08<00:00, 131MB/s]\n",
            "model.safetensors.index.json: 100% 63.5k/63.5k [00:00<00:00, 178MB/s]\n",
            "Loading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.10it/s]\n",
            "INFO 10-02 07:46:51 model_runner.py:1025] Loading model weights took 5.3748 GB\n",
            "INFO 10-02 07:46:53 gpu_executor.py:122] # GPU blocks: 14582, # CPU blocks: 2048\n",
            "INFO 10-02 07:46:55 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:46:55 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:47:18 model_runner.py:1456] Graph capturing finished in 23 secs.\n",
            "./data/en_input_data.jsonl - hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\n",
            "Processed prompts: 100% 541/541 [01:06<00:00,  8.14it/s, est. speed input: 667.12 toks/s, output: 3173.37 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 35.67ba/s]\n",
            "./data/es_input_data.jsonl - hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\n",
            "Processed prompts: 100% 115/115 [00:30<00:00,  3.80it/s, est. speed input: 299.41 toks/s, output: 2108.32 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 260.92ba/s]\n",
            "./data/fr_input_data.jsonl - hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\n",
            "Processed prompts: 100% 235/235 [00:41<00:00,  5.62it/s, est. speed input: 554.98 toks/s, output: 3136.57 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 118.37ba/s]\n",
            "./data/ja_input_data.jsonl - hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\n",
            "Processed prompts: 100% 172/172 [00:36<00:00,  4.70it/s, est. speed input: 421.09 toks/s, output: 2804.38 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 115.66ba/s]\n",
            "2024-10-02 07:50:25.931529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-02 07:50:25.949665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 07:50:25.971973: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 07:50:25.978620: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 07:50:25.995432: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 07:50:27.884314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 601/601 [00:00<00:00, 3.88MB/s]\n",
            "INFO 10-02 07:50:31 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "/usr/local/lib/python3.10/dist-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
            "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
            "tokenizer_config.json: 100% 141k/141k [00:00<00:00, 30.2MB/s]\n",
            "tokenizer.model: 100% 587k/587k [00:00<00:00, 40.0MB/s]\n",
            "tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 3.01MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.05MB/s]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 782kB/s]\n",
            "INFO 10-02 07:50:36 model_runner.py:1014] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
            "INFO 10-02 07:50:36 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "consolidated.safetensors: 100% 14.5G/14.5G [00:40<00:00, 362MB/s]\n",
            "model-00001-of-00003.safetensors: 100% 4.95G/4.95G [00:12<00:00, 387MB/s]\n",
            "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:13<00:00, 358MB/s]\n",
            "model-00003-of-00003.safetensors: 100% 4.55G/4.55G [00:15<00:00, 300MB/s]\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 80.7MB/s]\n",
            "Loading safetensors checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "INFO 10-02 07:52:06 model_runner.py:1025] Loading model weights took 13.5083 GB\n",
            "INFO 10-02 07:52:07 gpu_executor.py:122] # GPU blocks: 11005, # CPU blocks: 2048\n",
            "INFO 10-02 07:52:09 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-02 07:52:09 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-02 07:52:32 model_runner.py:1456] Graph capturing finished in 22 secs.\n",
            "./data/en_input_data.jsonl - mistralai/Mistral-7B-Instruct-v0.3\n",
            "Processed prompts: 100% 541/541 [01:13<00:00,  7.33it/s, est. speed input: 398.10 toks/s, output: 3056.00 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 78.87ba/s]\n",
            "./data/es_input_data.jsonl - mistralai/Mistral-7B-Instruct-v0.3\n",
            "Processed prompts: 100% 115/115 [00:27<00:00,  4.20it/s, est. speed input: 239.52 toks/s, output: 2183.86 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 283.36ba/s]\n",
            "./data/fr_input_data.jsonl - mistralai/Mistral-7B-Instruct-v0.3\n",
            "Processed prompts: 100% 235/235 [00:50<00:00,  4.69it/s, est. speed input: 358.09 toks/s, output: 2762.79 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 120.66ba/s]\n",
            "./data/ja_input_data.jsonl - mistralai/Mistral-7B-Instruct-v0.3\n",
            "Processed prompts: 100% 172/172 [00:56<00:00,  3.04it/s, est. speed input: 274.03 toks/s, output: 2730.73 toks/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 111.18ba/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSATCeaB_Na4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "pD0UrOo4-oOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd M-IFEval && pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "Xn4NZ6_d_Nc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4e41d6-c9c8-4da2-ffc5-4feb18602c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: M-IFEval: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "input_paths = glob(\"./data/*_input_data.jsonl\")\n",
        "\n",
        "for input_path in input_paths:\n",
        "    response_paths = glob(input_path[:-10] + \"response_data_*\")\n",
        "\n",
        "    for response_path in response_paths:\n",
        "        run_name = response_path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        ! mkdir -p ./evaluations/{run_name}\n",
        "        ! python -m evaluation_main \\\n",
        "          --input_data={input_path} \\\n",
        "          --input_response_data={response_path} \\\n",
        "          --output_dir=./evaluations/{run_name}\n"
      ],
      "metadata": {
        "id": "CjerXUbP_Ne2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "VXjZYqBt-yB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd M-IFEval\n",
        "# ! pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "lmwfE-eZ_Ng3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002c3777-ae3b-4ba4-bcfc-017d13a4bb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/M-IFEval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "paths = glob(\"./data/*_input_data.jsonl\")\n",
        "\n",
        "model_results = []\n",
        "\n",
        "for path in tqdm(paths):\n",
        "    lang_name = path.split(\"/\")[-1].split(\"_\")[0]\n",
        "    res_df = pd.read_json(path, lines=True)\n",
        "\n",
        "    print(lang_name)\n",
        "    print(res_df.shape)\n",
        "    print(res_df.instruction_id_list.str.len().value_counts())\n",
        "    print(res_df.instruction_id_list.explode().value_counts().min())\n",
        "\n",
        "    # exploded_res_df[\"model_name\"] = model_name[len(\"es_input_response_data_\"):]\n",
        "    # exploded_res_df[\"language\"] = model_name[:2]\n",
        "\n",
        "    # model_results.append(exploded_res_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621,
          "referenced_widgets": [
            "794980ba379e4b599ef2787c5c7f8a39",
            "f1dd83687e64486e8f17ef9a135389ec",
            "dea8b0265e5245268553e6239bb50646",
            "eeb1b17f59944675958d4d4d45541e39",
            "177c95e450af47db848f5dd8f95c538e",
            "2806c6c1dab74ccaa76dcdbcca1d03ec",
            "6085dee6e3464ac8b79654e9c595a305",
            "396863b1cda84846a33df00aa0c55b84",
            "f36fffa1e6d9488ebaf2d42b6be50112",
            "df8b71412c0d413d8d2c4cd09636d37d",
            "2a0466f1ab434f1caa15c7f0b9091a22"
          ]
        },
        "id": "gmfj2Dc2_LWM",
        "outputId": "943132ab-de9f-4f78-9301-f44fb72fcd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "794980ba379e4b599ef2787c5c7f8a39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "es\n",
            "(115, 4)\n",
            "instruction_id_list\n",
            "1    100\n",
            "2      8\n",
            "3      7\n",
            "Name: count, dtype: int64\n",
            "4\n",
            "ja\n",
            "(172, 4)\n",
            "instruction_id_list\n",
            "1    128\n",
            "2     34\n",
            "3     10\n",
            "Name: count, dtype: int64\n",
            "4\n",
            "fr\n",
            "(235, 4)\n",
            "instruction_id_list\n",
            "1    146\n",
            "2     68\n",
            "3     21\n",
            "Name: count, dtype: int64\n",
            "7\n",
            "en\n",
            "(541, 4)\n",
            "instruction_id_list\n",
            "1    305\n",
            "2    179\n",
            "3     57\n",
            "Name: count, dtype: int64\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "paths = glob(\"./evaluations/*/eval_results_strict.jsonl\")\n",
        "\n",
        "select_models = {\n",
        "    'gpt-4o-2024-08-06': \"GPT4o\",\n",
        "    'o1-mini-2024-09-12': \"o1 Mini\",\n",
        "    'claude-3-5-sonnet-20240620': \"Sonnet\",\n",
        "    'Qwen__Qwen2.5-32B-Instruct-GPTQ-Int4': \"Qwen 2.5 32B I.\",\n",
        "    'gpt-4o-mini-2024-07-18': \"GPT4o Mini\",\n",
        "    'claude-3-haiku-20240307': \"Haiku\",\n",
        "    'claude-3-opus-20240229': \"Opus\",\n",
        "    'o1-preview-2024-09-12': \"o1\"\n",
        "}\n",
        "\n",
        "model_results = []\n",
        "\n",
        "for path in tqdm(paths):\n",
        "    run_name = path.split(\"/\")[-2]\n",
        "    model_name = run_name[len(\"es_input_response_data_\"):]\n",
        "    if model_name not in select_models:\n",
        "        continue\n",
        "    res_df = pd.read_json(path, lines=True)\n",
        "    res_df[\"instr_len\"] = res_df.follow_instruction_list.str.len()\n",
        "\n",
        "    exploded_res_df = pd.DataFrame(res_df.apply(\n",
        "        lambda x: [\n",
        "            {\n",
        "                \"instruction_id\": inst, \"follow_bool\": val\n",
        "            } for inst, val in zip(x[\"instruction_id_list\"], x[\"follow_instruction_list\"])\n",
        "        ],\n",
        "        axis=1).explode())\n",
        "\n",
        "    exploded_res_df[\"instruction_id\"] = exploded_res_df[0].apply(lambda x: x[\"instruction_id\"])\n",
        "    exploded_res_df[\"follow_bool\"] = exploded_res_df[0].apply(lambda x: x[\"follow_bool\"])\n",
        "    exploded_res_df = exploded_res_df.drop(0, axis=1)\n",
        "    exploded_res_df[\"model_name\"] = select_models[model_name]\n",
        "    exploded_res_df[\"language\"] = run_name[:2]\n",
        "    idx_val_counts = exploded_res_df.index.value_counts()\n",
        "    exploded_res_df[\"num_instr\"] = exploded_res_df.index.map(idx_val_counts)\n",
        "\n",
        "    model_results.append(exploded_res_df)"
      ],
      "metadata": {
        "id": "meoJg0NG_Niv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7bdc4ca4e612470fb334c14f1c6f52bc",
            "5990b31bbec8474c89d2b9da95c17c11",
            "740baa4cc8e04333a035c14dd79d81e3",
            "9ad958599f3d48d1a897911077a61470",
            "19cfadd4b75d4a5891ba95ba6825a197",
            "863ceb79f6254ef3901945c5ae24626e",
            "ec5779060c2242beba5f03d9ae0500ef",
            "d2f5782df13b4e7c8676621e6a656a72",
            "a7593f64acfa4bcb9e639af4b6c8e6fc",
            "463032f19b234513b2cd46725bfd6ba9",
            "52849cde5f694898a8989da9b0c7aca1"
          ]
        },
        "outputId": "d920c955-dbae-46ca-d37b-2828deadb725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bdc4ca4e612470fb334c14f1c6f52bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_results_df = pd.concat(model_results)\n",
        "full_results_df[\"instruction_stem\"] = full_results_df.instruction_id.str.split(\":\").str[1:3].str.join(\":\")"
      ],
      "metadata": {
        "id": "iThFcLyE_Nnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per model analysis"
      ],
      "metadata": {
        "id": "G-jygHRkKT7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = full_results_df.groupby(\n",
        "    [\"model_name\", \"language\"]\n",
        "    ).follow_bool.mean().reset_index(drop=False).pivot(\n",
        "        index=\"model_name\", columns=\"language\", values=\"follow_bool\"\n",
        "        )"
      ],
      "metadata": {
        "id": "64h49zkMLA0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df[\"mean_esfrja\"] = grouped_df[[\"es\", \"fr\", \"ja\"]].mean(axis=1)"
      ],
      "metadata": {
        "id": "QuzpCLQaLA28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df.sort_values(\"mean_esfrja\", ascending=False).round(3) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "BJjgF6RzGDp-",
        "outputId": "64d40c02-c05a-41f9-bb94-8ea481b8360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "language           en    es    fr    ja  mean_esfrja\n",
              "model_name                                          \n",
              "o1               86.7  92.7  91.3  75.7         86.6\n",
              "Opus             87.3  90.5  87.0  75.7         84.4\n",
              "Sonnet           88.1  87.6  88.1  77.0         84.2\n",
              "o1 Mini          83.9  92.0  88.4  69.5         83.3\n",
              "GPT4o            88.6  89.8  87.8  70.4         82.7\n",
              "GPT4o Mini       86.0  85.4  85.5  65.9         78.9\n",
              "Qwen 2.5 32B I.  86.0  82.5  81.7  65.9         76.7\n",
              "Haiku            77.3  78.8  78.3  61.9         73.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b84fe4f7-88f6-4a56-99b5-d3f8592aa049\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>language</th>\n",
              "      <th>en</th>\n",
              "      <th>es</th>\n",
              "      <th>fr</th>\n",
              "      <th>ja</th>\n",
              "      <th>mean_esfrja</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>o1</th>\n",
              "      <td>86.7</td>\n",
              "      <td>92.7</td>\n",
              "      <td>91.3</td>\n",
              "      <td>75.7</td>\n",
              "      <td>86.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Opus</th>\n",
              "      <td>87.3</td>\n",
              "      <td>90.5</td>\n",
              "      <td>87.0</td>\n",
              "      <td>75.7</td>\n",
              "      <td>84.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sonnet</th>\n",
              "      <td>88.1</td>\n",
              "      <td>87.6</td>\n",
              "      <td>88.1</td>\n",
              "      <td>77.0</td>\n",
              "      <td>84.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>o1 Mini</th>\n",
              "      <td>83.9</td>\n",
              "      <td>92.0</td>\n",
              "      <td>88.4</td>\n",
              "      <td>69.5</td>\n",
              "      <td>83.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT4o</th>\n",
              "      <td>88.6</td>\n",
              "      <td>89.8</td>\n",
              "      <td>87.8</td>\n",
              "      <td>70.4</td>\n",
              "      <td>82.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT4o Mini</th>\n",
              "      <td>86.0</td>\n",
              "      <td>85.4</td>\n",
              "      <td>85.5</td>\n",
              "      <td>65.9</td>\n",
              "      <td>78.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qwen 2.5 32B I.</th>\n",
              "      <td>86.0</td>\n",
              "      <td>82.5</td>\n",
              "      <td>81.7</td>\n",
              "      <td>65.9</td>\n",
              "      <td>76.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Haiku</th>\n",
              "      <td>77.3</td>\n",
              "      <td>78.8</td>\n",
              "      <td>78.3</td>\n",
              "      <td>61.9</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b84fe4f7-88f6-4a56-99b5-d3f8592aa049')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b84fe4f7-88f6-4a56-99b5-d3f8592aa049 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b84fe4f7-88f6-4a56-99b5-d3f8592aa049');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13866627-7e84-4233-be38-b698ceb67449\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13866627-7e84-4233-be38-b698ceb67449')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13866627-7e84-4233-be38-b698ceb67449 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"grouped_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Opus\",\n          \"GPT4o Mini\",\n          \"o1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"en\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.612453657636514,\n        \"min\": 77.3,\n        \"max\": 88.6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          86.7,\n          87.3,\n          86.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"es\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.8789452900045065,\n        \"min\": 78.8,\n        \"max\": 92.7,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          90.5,\n          85.39999999999999,\n          92.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.149505134006087,\n        \"min\": 78.3,\n        \"max\": 91.3,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          87.0,\n          85.5,\n          91.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ja\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.5177635220906565,\n        \"min\": 61.9,\n        \"max\": 77.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          75.7,\n          77.0,\n          61.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_esfrja\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.595572403334071,\n        \"min\": 73.0,\n        \"max\": 86.6,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          84.39999999999999,\n          78.9,\n          86.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per model (only unique categories)"
      ],
      "metadata": {
        "id": "fxa_qQHGKjA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_instruction_list = ['detectable_content:informal_address',\n",
        " 'detectable_content:no_digits',\n",
        " 'detectable_format:nominal_ending',\n",
        " 'detectable_format:number_numbered_lists',\n",
        " 'length_constraints:number_letters',\n",
        " 'letters:furigana',\n",
        " 'letters:hiragana_only',\n",
        " 'letters:kanji',\n",
        " 'letters:kansuuji',\n",
        " 'letters:katakana_only',\n",
        " 'letters:no_hiragana',\n",
        " 'letters:no_katakana',\n",
        " 'punctuation:exclamation_marks',\n",
        " 'punctuation:no_period',\n",
        " 'punctuation:question_marks',\n",
        " 'special_character:accents',\n",
        " 'special_character:dieresis',\n",
        " 'special_character:enie',\n",
        " 'special_character:ethel_or_cedilla',\n",
        " 'special_character:no_accents',\n",
        " 'special_character:tildes',\n",
        " 'startend:sentence_unified_end']\n",
        "\n",
        "grouped_df = full_results_df[full_results_df.instruction_stem.isin(unique_instruction_list)].groupby(\n",
        "    [\"model_name\", \"language\"]\n",
        "    ).follow_bool.mean().reset_index(drop=False).pivot(\n",
        "        index=\"model_name\", columns=\"language\", values=\"follow_bool\"\n",
        "        )\n",
        "\n",
        "grouped_df[\"mean_esfrja\"] = grouped_df[[\"es\", \"fr\", \"ja\"]].mean(axis=1)"
      ],
      "metadata": {
        "id": "_bDmM_mMKb00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df.sort_values(\"mean_esfrja\", ascending=False).round(3) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "7r4PShNEKzhj",
        "outputId": "449e0288-20b7-4a9f-8fe6-7ccaf0225620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "language           es    fr    ja  mean_esfrja\n",
              "model_name                                    \n",
              "o1               75.0  96.1  61.4         77.5\n",
              "Sonnet           66.7  90.2  70.5         75.8\n",
              "Opus             62.5  90.2  64.8         72.5\n",
              "GPT4o            58.3  80.4  55.7         64.8\n",
              "o1 Mini          66.7  72.5  50.0         63.1\n",
              "Qwen 2.5 32B I.  54.2  78.4  54.5         62.4\n",
              "Haiku            54.2  80.4  52.3         62.3\n",
              "GPT4o Mini       58.3  68.6  47.7         58.2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-607b64b6-18fd-4293-ba86-077653e612d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>language</th>\n",
              "      <th>es</th>\n",
              "      <th>fr</th>\n",
              "      <th>ja</th>\n",
              "      <th>mean_esfrja</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>o1</th>\n",
              "      <td>75.0</td>\n",
              "      <td>96.1</td>\n",
              "      <td>61.4</td>\n",
              "      <td>77.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sonnet</th>\n",
              "      <td>66.7</td>\n",
              "      <td>90.2</td>\n",
              "      <td>70.5</td>\n",
              "      <td>75.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Opus</th>\n",
              "      <td>62.5</td>\n",
              "      <td>90.2</td>\n",
              "      <td>64.8</td>\n",
              "      <td>72.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT4o</th>\n",
              "      <td>58.3</td>\n",
              "      <td>80.4</td>\n",
              "      <td>55.7</td>\n",
              "      <td>64.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>o1 Mini</th>\n",
              "      <td>66.7</td>\n",
              "      <td>72.5</td>\n",
              "      <td>50.0</td>\n",
              "      <td>63.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qwen 2.5 32B I.</th>\n",
              "      <td>54.2</td>\n",
              "      <td>78.4</td>\n",
              "      <td>54.5</td>\n",
              "      <td>62.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Haiku</th>\n",
              "      <td>54.2</td>\n",
              "      <td>80.4</td>\n",
              "      <td>52.3</td>\n",
              "      <td>62.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT4o Mini</th>\n",
              "      <td>58.3</td>\n",
              "      <td>68.6</td>\n",
              "      <td>47.7</td>\n",
              "      <td>58.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-607b64b6-18fd-4293-ba86-077653e612d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-607b64b6-18fd-4293-ba86-077653e612d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-607b64b6-18fd-4293-ba86-077653e612d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-01ad5e19-1a6a-451f-8daf-6c1d21121e08\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01ad5e19-1a6a-451f-8daf-6c1d21121e08')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-01ad5e19-1a6a-451f-8daf-6c1d21121e08 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"grouped_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Sonnet\",\n          \"Qwen 2.5 32B I.\",\n          \"o1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"es\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.196117504797788,\n        \"min\": 54.2,\n        \"max\": 75.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          66.7,\n          54.2,\n          62.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.419584461565774,\n        \"min\": 68.60000000000001,\n        \"max\": 96.1,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          96.1,\n          90.2,\n          68.60000000000001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ja\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.818190784501049,\n        \"min\": 47.699999999999996,\n        \"max\": 70.5,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          70.5,\n          54.50000000000001,\n          61.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_esfrja\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.157763018469628,\n        \"min\": 58.199999999999996,\n        \"max\": 77.5,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          75.8,\n          62.4,\n          77.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per model (only non-unique)"
      ],
      "metadata": {
        "id": "Mc5uMGxnFrOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = full_results_df[~full_results_df.instruction_stem.isin(unique_instruction_list)].groupby(\n",
        "    [\"model_name\", \"language\"]\n",
        "    ).follow_bool.mean().reset_index(drop=False).pivot(\n",
        "        index=\"model_name\", columns=\"language\", values=\"follow_bool\"\n",
        "        )\n",
        "\n",
        "grouped_df[\"mean_esfrja\"] = grouped_df[[\"es\", \"fr\", \"ja\"]].mean(axis=1)"
      ],
      "metadata": {
        "id": "KnCTv5sAFwJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df.sort_values(\"mean_esfrja\", ascending=False).round(3) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "1Sh7HpGrFyU1",
        "outputId": "ffff2754-b341-4a18-d94c-dd9ae6b4b814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "language           en    es    fr    ja  mean_esfrja\n",
              "model_name                                          \n",
              "o1               86.7  96.5  90.5  84.8         90.6\n",
              "o1 Mini          83.9  97.3  91.2  81.9         90.1\n",
              "Opus             87.3  96.5  86.4  82.6         88.5\n",
              "GPT4o            88.6  96.5  89.1  79.7         88.4\n",
              "Sonnet           88.1  92.0  87.8  81.2         87.0\n",
              "GPT4o Mini       86.0  91.2  88.4  77.5         85.7\n",
              "Qwen 2.5 32B I.  86.0  88.5  82.3  73.2         81.3\n",
              "Haiku            77.3  84.1  77.9  68.1         76.7"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-244323e5-f3ab-4c89-b5c6-83ff1e5893c2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>language</th>\n",
              "      <th>en</th>\n",
              "      <th>es</th>\n",
              "      <th>fr</th>\n",
              "      <th>ja</th>\n",
              "      <th>mean_esfrja</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>o1</th>\n",
              "      <td>86.7</td>\n",
              "      <td>96.5</td>\n",
              "      <td>90.5</td>\n",
              "      <td>84.8</td>\n",
              "      <td>90.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>o1 Mini</th>\n",
              "      <td>83.9</td>\n",
              "      <td>97.3</td>\n",
              "      <td>91.2</td>\n",
              "      <td>81.9</td>\n",
              "      <td>90.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Opus</th>\n",
              "      <td>87.3</td>\n",
              "      <td>96.5</td>\n",
              "      <td>86.4</td>\n",
              "      <td>82.6</td>\n",
              "      <td>88.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT4o</th>\n",
              "      <td>88.6</td>\n",
              "      <td>96.5</td>\n",
              "      <td>89.1</td>\n",
              "      <td>79.7</td>\n",
              "      <td>88.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sonnet</th>\n",
              "      <td>88.1</td>\n",
              "      <td>92.0</td>\n",
              "      <td>87.8</td>\n",
              "      <td>81.2</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT4o Mini</th>\n",
              "      <td>86.0</td>\n",
              "      <td>91.2</td>\n",
              "      <td>88.4</td>\n",
              "      <td>77.5</td>\n",
              "      <td>85.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qwen 2.5 32B I.</th>\n",
              "      <td>86.0</td>\n",
              "      <td>88.5</td>\n",
              "      <td>82.3</td>\n",
              "      <td>73.2</td>\n",
              "      <td>81.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Haiku</th>\n",
              "      <td>77.3</td>\n",
              "      <td>84.1</td>\n",
              "      <td>77.9</td>\n",
              "      <td>68.1</td>\n",
              "      <td>76.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-244323e5-f3ab-4c89-b5c6-83ff1e5893c2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-244323e5-f3ab-4c89-b5c6-83ff1e5893c2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-244323e5-f3ab-4c89-b5c6-83ff1e5893c2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f43957ab-4597-4a16-aaa0-dfe5a458620f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f43957ab-4597-4a16-aaa0-dfe5a458620f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f43957ab-4597-4a16-aaa0-dfe5a458620f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"grouped_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"o1 Mini\",\n          \"GPT4o Mini\",\n          \"o1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"en\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.612453657636514,\n        \"min\": 77.3,\n        \"max\": 88.6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          86.7,\n          83.89999999999999,\n          86.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"es\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.761377351745427,\n        \"min\": 84.1,\n        \"max\": 97.3,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          96.5,\n          97.3,\n          84.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.495076671839344,\n        \"min\": 77.9,\n        \"max\": 91.2,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          91.2,\n          88.4,\n          90.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ja\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.528045379181114,\n        \"min\": 68.10000000000001,\n        \"max\": 84.8,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          81.89999999999999,\n          77.5,\n          84.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_esfrja\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.7755141832357415,\n        \"min\": 76.7,\n        \"max\": 90.60000000000001,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          90.10000000000001,\n          85.7,\n          90.60000000000001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per instruction results"
      ],
      "metadata": {
        "id": "xpnvBwMCKYTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = full_results_df.groupby(\n",
        "    [\"model_name\", \"instruction_stem\"]\n",
        "    ).follow_bool.mean().reset_index(drop=False).pivot(\n",
        "        index=\"model_name\", columns=\"instruction_stem\", values=\"follow_bool\"\n",
        "        )"
      ],
      "metadata": {
        "id": "jFtsPAzWGcZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_map = full_results_df.groupby([\"instruction_stem\"]).language.apply(set)\n",
        "\n",
        "per_instr_scores = grouped_df.T.mean(axis=1).sort_index().reset_index(drop=False).join(\n",
        "    lang_map.sort_index().reset_index(drop=True)\n",
        "    ).sort_values(0, ascending=True)"
      ],
      "metadata": {
        "id": "9e8IBG_QHALm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_instr_scores[\"instruction_family\"] = per_instr_scores[\"instruction_stem\"].str.split(\":\").str[0].str.replace(\"_\", \" \").str.capitalize()\n",
        "per_instr_scores[\"instruction_name\"] = per_instr_scores[\"instruction_stem\"].str.split(\":\").str[1].str.replace(\"_\", \" \").str.capitalize()\n",
        "per_instr_scores[\"language\"] = per_instr_scores[\"language\"].apply(lambda x: \", \".join([y.upper() for y in x]))\n",
        "\n",
        "per_instr_scores[[\"instruction_family\", \"instruction_name\", \"language\", 0]]"
      ],
      "metadata": {
        "id": "fQTZEXmfG1dD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3779e97a-8722-4564-9489-b990e4be5a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    instruction_family             instruction_name        language         0\n",
              "44   Special character                         Enie              ES  0.000000\n",
              "37             Letters                  No katakana              JA  0.142857\n",
              "31             Letters                     Furigana              JA  0.145833\n",
              "43   Special character                     Dieresis              ES  0.156250\n",
              "50            Startend         Sentence unified end              JA  0.339286\n",
              "36             Letters                  No hiragana              JA  0.357143\n",
              "32             Letters                Hiragana only              JA  0.482143\n",
              "45   Special character             Ethel or cedilla              FR  0.602273\n",
              "35             Letters                Katakana only              JA  0.604167\n",
              "46   Special character                   No accents              FR  0.637500\n",
              "16   Detectable format               Nominal ending              JA  0.642857\n",
              "14   Detectable format                  Json format  EN, ES, FR, JA  0.675000\n",
              "26  Length constraints     Nth paragraph first word  EN, ES, FR, JA  0.676471\n",
              "24            Keywords             Letter frequency  EN, ES, FR, JA  0.712054\n",
              "3          Change case               French capital              FR  0.734375\n",
              "33             Letters                        Kanji              JA  0.750000\n",
              "29  Length constraints             Number sentences  EN, ES, FR, JA  0.763889\n",
              "7          Combination                Repeat prompt  EN, ES, FR, JA  0.781780\n",
              "17   Detectable format          Number bullet lists  EN, ES, FR, JA  0.792453\n",
              "47   Special character                       Tildes              ES  0.796875\n",
              "0          Change case       Capital word frequency      EN, ES, FR  0.801136\n",
              "28  Length constraints            Number paragraphs  EN, ES, FR, JA  0.805288\n",
              "30  Length constraints                 Number words      EN, ES, FR  0.822368\n",
              "22            Keywords              Forbidden words  EN, ES, FR, JA  0.841216\n",
              "23            Keywords                    Frequency  EN, ES, FR, JA  0.852273\n",
              "49            Startend                    Quotation  EN, ES, FR, JA  0.852459\n",
              "1          Change case              English capital              EN  0.860000\n",
              "2          Change case            English lowercase              EN  0.881410\n",
              "8          Combination                Two responses  EN, ES, FR, JA  0.882979\n",
              "27  Length constraints               Number letters              JA  0.892857\n",
              "21            Keywords                    Existence  EN, ES, FR, JA  0.896484\n",
              "39         Punctuation                     No comma  EN, ES, FR, JA  0.901685\n",
              "9   Detectable content             Informal address              FR  0.909091\n",
              "34             Letters                     Kansuuji              JA  0.910714\n",
              "48            Startend                  End checker  EN, ES, FR, JA  0.913462\n",
              "20   Detectable format                        Title  EN, ES, FR, JA  0.923387\n",
              "40         Punctuation                    No period              JA  0.928571\n",
              "11  Detectable content          Number placeholders  EN, ES, FR, JA  0.928571\n",
              "13   Detectable format         Constrained response  EN, ES, FR, JA  0.932692\n",
              "18   Detectable format  Number highlighted sections  EN, ES, FR, JA  0.948214\n",
              "25            Language            Response language  EN, ES, FR, JA  0.955729\n",
              "19   Detectable format        Number numbered lists              JA  0.964286\n",
              "38         Punctuation            Exclamation marks              ES  0.968750\n",
              "15   Detectable format            Multiple sections  EN, ES, FR, JA  0.975694\n",
              "12  Detectable content                   Postscript  EN, ES, FR, JA  0.977500\n",
              "42   Special character                      Accents              FR  0.982143\n",
              "41         Punctuation               Question marks              ES  1.000000\n",
              "6          Change case            Spanish lowercase              ES  1.000000\n",
              "5          Change case              Spanish capital              ES  1.000000\n",
              "4          Change case             French lowercase              FR  1.000000\n",
              "10  Detectable content                    No digits              FR  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db2e6f77-989d-4abe-99b1-ee1d15aebe2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction_family</th>\n",
              "      <th>instruction_name</th>\n",
              "      <th>language</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Special character</td>\n",
              "      <td>Enie</td>\n",
              "      <td>ES</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Letters</td>\n",
              "      <td>No katakana</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Letters</td>\n",
              "      <td>Furigana</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.145833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Special character</td>\n",
              "      <td>Dieresis</td>\n",
              "      <td>ES</td>\n",
              "      <td>0.156250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Startend</td>\n",
              "      <td>Sentence unified end</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.339286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Letters</td>\n",
              "      <td>No hiragana</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Letters</td>\n",
              "      <td>Hiragana only</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.482143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Special character</td>\n",
              "      <td>Ethel or cedilla</td>\n",
              "      <td>FR</td>\n",
              "      <td>0.602273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Letters</td>\n",
              "      <td>Katakana only</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.604167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Special character</td>\n",
              "      <td>No accents</td>\n",
              "      <td>FR</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Nominal ending</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Json format</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Length constraints</td>\n",
              "      <td>Nth paragraph first word</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.676471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Keywords</td>\n",
              "      <td>Letter frequency</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.712054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Change case</td>\n",
              "      <td>French capital</td>\n",
              "      <td>FR</td>\n",
              "      <td>0.734375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Letters</td>\n",
              "      <td>Kanji</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Length constraints</td>\n",
              "      <td>Number sentences</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.763889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Combination</td>\n",
              "      <td>Repeat prompt</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.781780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Number bullet lists</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.792453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Special character</td>\n",
              "      <td>Tildes</td>\n",
              "      <td>ES</td>\n",
              "      <td>0.796875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Change case</td>\n",
              "      <td>Capital word frequency</td>\n",
              "      <td>EN, ES, FR</td>\n",
              "      <td>0.801136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Length constraints</td>\n",
              "      <td>Number paragraphs</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.805288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Length constraints</td>\n",
              "      <td>Number words</td>\n",
              "      <td>EN, ES, FR</td>\n",
              "      <td>0.822368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Keywords</td>\n",
              "      <td>Forbidden words</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.841216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Keywords</td>\n",
              "      <td>Frequency</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.852273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Startend</td>\n",
              "      <td>Quotation</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.852459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Change case</td>\n",
              "      <td>English capital</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Change case</td>\n",
              "      <td>English lowercase</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.881410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Combination</td>\n",
              "      <td>Two responses</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.882979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Length constraints</td>\n",
              "      <td>Number letters</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.892857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Keywords</td>\n",
              "      <td>Existence</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.896484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Punctuation</td>\n",
              "      <td>No comma</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.901685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Detectable content</td>\n",
              "      <td>Informal address</td>\n",
              "      <td>FR</td>\n",
              "      <td>0.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Letters</td>\n",
              "      <td>Kansuuji</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.910714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Startend</td>\n",
              "      <td>End checker</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.913462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Title</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.923387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Punctuation</td>\n",
              "      <td>No period</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Detectable content</td>\n",
              "      <td>Number placeholders</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Constrained response</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.932692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Number highlighted sections</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.948214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Language</td>\n",
              "      <td>Response language</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.955729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Number numbered lists</td>\n",
              "      <td>JA</td>\n",
              "      <td>0.964286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Punctuation</td>\n",
              "      <td>Exclamation marks</td>\n",
              "      <td>ES</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Detectable format</td>\n",
              "      <td>Multiple sections</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.975694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Detectable content</td>\n",
              "      <td>Postscript</td>\n",
              "      <td>EN, ES, FR, JA</td>\n",
              "      <td>0.977500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Special character</td>\n",
              "      <td>Accents</td>\n",
              "      <td>FR</td>\n",
              "      <td>0.982143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Punctuation</td>\n",
              "      <td>Question marks</td>\n",
              "      <td>ES</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Change case</td>\n",
              "      <td>Spanish lowercase</td>\n",
              "      <td>ES</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Change case</td>\n",
              "      <td>Spanish capital</td>\n",
              "      <td>ES</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Change case</td>\n",
              "      <td>French lowercase</td>\n",
              "      <td>FR</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Detectable content</td>\n",
              "      <td>No digits</td>\n",
              "      <td>FR</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db2e6f77-989d-4abe-99b1-ee1d15aebe2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-db2e6f77-989d-4abe-99b1-ee1d15aebe2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-db2e6f77-989d-4abe-99b1-ee1d15aebe2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b24658c3-bbcb-4c33-bf94-378efcd4d9d9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b24658c3-bbcb-4c33-bf94-378efcd4d9d9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b24658c3-bbcb-4c33-bf94-378efcd4d9d9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"per_instr_scores[[\\\"instruction_family\\\", \\\"instruction_name\\\", \\\"language\\\", 0]]\",\n  \"rows\": 51,\n  \"fields\": [\n    {\n      \"column\": \"instruction_family\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Keywords\",\n          \"Special character\",\n          \"Detectable content\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instruction_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          \"Multiple sections\",\n          \"Response language\",\n          \"Question marks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"ES\",\n          \"JA\",\n          \"EN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2493407978254029,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          0.8814102564102564,\n          0.9482142857142858,\n          0.86\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHNNqLVAcbkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}